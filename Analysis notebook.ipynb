{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fe4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary # DEBUG\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.dataset_loaders import *\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import UNet3DConditionModel\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b83fb",
   "metadata": {},
   "source": [
    "Instruments for manual noising and denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195077d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# А нужно ли?\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mCorrellatedNoiseVideoScheduler\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;18;43m__slots__\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbetas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malphas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malphas_cumprod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-2\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m, in \u001b[0;36mCorrellatedNoiseVideoScheduler\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     noisy_samples \u001b[38;5;241m=\u001b[39m sqrt_alpha_prod \u001b[38;5;241m*\u001b[39m original_samples \u001b[38;5;241m+\u001b[39m sqrt_one_minus_alpha_prod \u001b[38;5;241m*\u001b[39m noise\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m noisy_samples\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     41\u001b[0m     model_output: torch\u001b[38;5;241m.\u001b[39mFloatTensor,\n\u001b[1;32m     42\u001b[0m     timestep: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     43\u001b[0m     sample: torch\u001b[38;5;241m.\u001b[39mFloatTensor,\n\u001b[1;32m     44\u001b[0m     generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     45\u001b[0m     return_dict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m---> 46\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mUnion\u001b[49m[DDPMSchedulerOutput, Tuple]:\n\u001b[1;32m     47\u001b[0m     t \u001b[38;5;241m=\u001b[39m timestep\n\u001b[1;32m     49\u001b[0m     prev_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevious_timestep(t)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Union' is not defined"
     ]
    }
   ],
   "source": [
    "# А нужно ли?\n",
    "\n",
    "class CorrellatedNoiseVideoScheduler():\n",
    "    __slots__ = \"betas\", \"alphas\", \"alphas_cumprod\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_steps=1000,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=2e-2\n",
    "    ):\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_steps, dtype=torch.float32)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "    \n",
    "    def add_noise(\n",
    "        self,\n",
    "        original_samples: torch.FloatTensor,\n",
    "        noise: torch.FloatTensor,\n",
    "        timesteps: torch.IntTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n",
    "        alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n",
    "        timesteps = timesteps.to(original_samples.device)\n",
    "\n",
    "        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n",
    "        return noisy_samples\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        model_output: torch.FloatTensor,\n",
    "        timestep: int,\n",
    "        sample: torch.FloatTensor,\n",
    "        generator=None,\n",
    "        return_dict: bool = True,\n",
    "    ) -> Union[DDPMSchedulerOutput, Tuple]:\n",
    "        t = timestep\n",
    "\n",
    "        prev_t = self.previous_timestep(t)\n",
    "\n",
    "        if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in [\"learned\", \"learned_range\"]:\n",
    "            model_output, predicted_variance = torch.split(model_output, sample.shape[1], dim=1)\n",
    "        else:\n",
    "            predicted_variance = None\n",
    "\n",
    "        # 1. compute alphas, betas\n",
    "        alpha_prod_t = self.alphas_cumprod[t]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "        current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n",
    "        current_beta_t = 1 - current_alpha_t\n",
    "\n",
    "        # 2. compute predicted original sample from predicted noise also called\n",
    "        # \"predicted x_0\" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "\n",
    "        # 3. Clip or threshold \"predicted x_0\"\n",
    "        if self.config.thresholding:\n",
    "            pred_original_sample = self._threshold_sample(pred_original_sample)\n",
    "        elif self.config.clip_sample:\n",
    "            pred_original_sample = pred_original_sample.clamp(\n",
    "                -self.config.clip_sample_range, self.config.clip_sample_range\n",
    "            )\n",
    "\n",
    "        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * current_beta_t) / beta_prod_t\n",
    "        current_sample_coeff = current_alpha_t ** (0.5) * beta_prod_t_prev / beta_prod_t\n",
    "\n",
    "        # 5. Compute predicted previous sample µ_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample\n",
    "\n",
    "        # 6. Add noise\n",
    "        variance = 0\n",
    "        if t > 0:\n",
    "            device = model_output.device\n",
    "            variance_noise = randn_tensor(\n",
    "                model_output.shape, generator=generator, device=device, dtype=model_output.dtype\n",
    "            )\n",
    "            if self.variance_type == \"fixed_small_log\":\n",
    "                variance = self._get_variance(t, predicted_variance=predicted_variance) * variance_noise\n",
    "            elif self.variance_type == \"learned_range\":\n",
    "                variance = self._get_variance(t, predicted_variance=predicted_variance)\n",
    "                variance = torch.exp(0.5 * variance) * variance_noise\n",
    "            else:\n",
    "                variance = (self._get_variance(t, predicted_variance=predicted_variance) ** 0.5) * variance_noise\n",
    "\n",
    "        pred_prev_sample = pred_prev_sample + variance\n",
    "\n",
    "        if not return_dict:\n",
    "            return (pred_prev_sample,)\n",
    "\n",
    "        return DDPMSchedulerOutput(prev_sample=pred_prev_sample, pred_original_sample=pred_original_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b6dff",
   "metadata": {},
   "source": [
    "Creating dataset and dataloader for UCF-101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb5c4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "UCF_dataset = UCFDataset(\"./datasets/UCF-101/\")\n",
    "\n",
    "batch_size = 1\n",
    "UCF_dataloader = DataLoader(UCF_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe366fbf",
   "metadata": {},
   "source": [
    "Trying default DDPMScheduler for working with videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387215d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_steps = 1000\n",
    "\n",
    "def train_simple_new(\n",
    "    model,\n",
    "    dataloader,\n",
    "    noise_scheduler,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    criterion,\n",
    "    num_epochs,\n",
    "    device=\"cuda:0\",\n",
    "    noise_cov=lambda x: torch.eye(x),\n",
    "):\n",
    "    \"\"\"\n",
    "    noise_cov -- matrix with the shape of video length or callable that receives video length and \n",
    "                 returns matrix\n",
    "    \"\"\"\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, (videos, _) in enumerate(pbar):\n",
    "            videos = videos.to(device)\n",
    "            steps = torch.randint(low=0, high=total_num_steps + 1, size=(videos.shape[0],), device=device)\n",
    "            if callable(noise_cov):\n",
    "                noise_gen = MultivarNorm(cov_matrix = noise_cov(videos.shape[1]))\n",
    "            else:\n",
    "                noise_gen = MultivarNorm(cov_matrix = noise_cov)\n",
    "            noise = noise_gen.sample(videos.shape)\n",
    "            noised_videos = noise_scheduler.add_noise(videos, noise, steps)\n",
    "            predicted_noise = model(noised_videos, steps.to(device)).sample\n",
    "            loss = criterion(noise, predicted_noise)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            pbar.set_postfix(MSE=loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def init_training_modules_new(\n",
    "    lr_warmup_steps,\n",
    "    num_epochs,\n",
    "    device=\"cuda:0\"\n",
    "):\n",
    "    model = UNet3DConditionModel(\n",
    "        sample_size=img_size,\n",
    "        in_channels=4,\n",
    "        out_channels=3,\n",
    "        layers_per_block=2,\n",
    "        block_out_channels=(64, 128),\n",
    "        down_block_types=(\n",
    "            \"CrossAttnDownBlock3D\",\n",
    "            \"DownBlock3D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock3D\",\n",
    "            \"CrossAttnUpBlock3D\",\n",
    "          ),\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        num_train_timesteps=total_num_steps, beta_start=beta_start, beta_end=beta_end)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=lr_warmup_steps,\n",
    "        num_training_steps=(len(dataloader) * num_epochs),\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    output = (model, noise_scheduler, optimizer, lr_scheduler, criterion)\n",
    "\n",
    "    return output\n",
    "\n",
    "def sample_videos(\n",
    "    model,\n",
    "    num_videos,\n",
    "    video_length,\n",
    "    prompts\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(num_videos, video_length, 1, img_size, img_size).to(device)\n",
    "        for i, t in enumerate(noise_scheduler.timesteps):\n",
    "            residual = model(sample, t).sample\n",
    "            sample = noise_scheduler.step(residual, t, sample).prev_sample\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19fb192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet3DConditionModel(\n",
    "        sample_size=(240, 320),\n",
    "        in_channels=3,\n",
    "        out_channels=3,\n",
    "        layers_per_block=2,\n",
    "        block_out_channels=(32, 32),\n",
    "        down_block_types=(\n",
    "            \"CrossAttnDownBlock3D\",\n",
    "            \"DownBlock3D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock3D\",\n",
    "            \"CrossAttnUpBlock3D\",\n",
    "          ),\n",
    "    )\n",
    "# model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d84c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb59432",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    model,\n",
    "    input_data = {\n",
    "        \"sample\": torch.randn(1, 3, 30, 240, 320),\n",
    "        \"timestep\": 500,\n",
    "        \"encoder_hidden_states\": torch.ones(1, 200, 1) * 3.0,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
