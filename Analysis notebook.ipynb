{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fe4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koltakovmi/.local/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/koltakovmi/.local/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.noise_gen import *\n",
    "from utils.training import *\n",
    "from utils.dataset_loaders import *\n",
    "from models.basic_models import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import write_video, write_png\n",
    "from diffusers import UNet3DConditionModel, DDPMScheduler, DDPMPipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580790a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b6dff",
   "metadata": {},
   "source": [
    "Creating dataset and dataloader for UCF-101 and MovingMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa625426",
   "metadata": {},
   "outputs": [],
   "source": [
    "MovMNIST_frame_dataset = MovMNISTFrameDataset(\"./datasets/moving_mnist_labeled/\")\n",
    "MovMNIST_frame_dataloader = DataLoader(MovMNIST_frame_dataset, shuffle=True, batch_size=72)\n",
    "\n",
    "dev = \"cuda:5\"\n",
    "\n",
    "model_frame, noise_scheduler_frame, optimizer_frame, lr_scheduler_frame, criterion_frame = init_mov_mnist_model(\n",
    "    lr_warmup_steps=100,\n",
    "    num_epochs=4,\n",
    "    beta_start=1.17e-3,\n",
    "    beta_end=1.88e-1,\n",
    "    object_cnt = len(MovMNIST_frame_dataloader),\n",
    "    device=dev,\n",
    "    model_type=\"image\",\n",
    "    use_labels=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6edd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "MovMNIST_dataset = MovMNISTDataset(\"./datasets/moving_mnist_labeled/\")\n",
    "MovMNIST_dataloader = DataLoader(MovMNIST_dataset, shuffle=True, batch_size=2)\n",
    "\n",
    "dev = \"cuda:1\"\n",
    "\n",
    "model_video, noise_scheduler_video, optimizer_video, lr_scheduler_video, criterion_video = init_mov_mnist_model(\n",
    "    lr_warmup_steps=100,\n",
    "    num_epochs=4,\n",
    "    beta_start=1.17e-3,\n",
    "    beta_end=1.88e-1,\n",
    "    object_cnt = len(MovMNIST_dataloader),\n",
    "    device=dev,\n",
    "    model_type=\"video\",\n",
    "    use_labels=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1a03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_image = TrainableDiffusionModel(\n",
    "    model_ref = model_frame,\n",
    "    optimizer_ref = optimizer_frame,\n",
    "    lr_scheduler_ref=lr_scheduler_frame,\n",
    "    noise_scheduler = noise_scheduler_frame,\n",
    "    criterion = criterion_frame,\n",
    "    device=\"cuda:1\",\n",
    "    model_type=\"image\",\n",
    "    cross_att_dim=4,\n",
    "    EMA_start=2500,\n",
    ")\n",
    "\n",
    "trainer_image.load_state(base_dir_path=\"./models/trained/mov_mnist_frames_batch96/\", suffix=\"8000\",\n",
    "                   load_optimizer=False, load_lr_sched=False, load_ema_model=False)\n",
    "\n",
    "# test_losses = trainer.fit(\n",
    "#     dataloader = MovMNIST_frame_dataloader,\n",
    "#     save_path = \"./models/trained/mov_mnist_frames_batch96/\",\n",
    "#     num_epochs = 4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c100056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                             | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                                                                                                                        | 5/5000 [00:29<7:51:11,  5.66s/it, MSE=1.24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                                                                                                                        | 5/5000 [00:29<8:10:24,  5.89s/it, MSE=1.24]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/koltakovmi/diffusion_noise_scheduling/Analysis notebook.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer_video \u001b[39m=\u001b[39m TrainableDiffusionModel(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model_ref \u001b[39m=\u001b[39m model_video,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     optimizer_ref \u001b[39m=\u001b[39m optimizer_video,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     EMA_start\u001b[39m=\u001b[39m\u001b[39m7500\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# trainer_video.load_weights_from(trainer_image.model_ref)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# trainer_video.load_weights_from(trainer_image.model_ref, other_type=\"ema_model\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m test_losses \u001b[39m=\u001b[39m trainer_video\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     dataloader \u001b[39m=\u001b[39;49m MovMNIST_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     save_path \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m./models/trained/test/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     num_epochs \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m )\n",
      "File \u001b[0;32m~/diffusion_noise_scheduling/utils/training.py:254\u001b[0m, in \u001b[0;36mTrainableDiffusionModel.fit\u001b[0;34m(self, dataloader, save_path, num_epochs, end_processor, grad_accum_steps, class_free_guidance_threshhold)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39m# Going through epochs\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 254\u001b[0m \tnew_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_one_epoch(dataloader, end_proc, class_free_guidance_threshhold\u001b[39m=\u001b[39;49mclass_free_guidance_threshhold)\n\u001b[1;32m    255\u001b[0m \tlosses \u001b[39m=\u001b[39m cat([losses, as_tensor(new_losses)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[39m# saving last versions of models\u001b[39;00m\n",
      "File \u001b[0;32m~/diffusion_noise_scheduling/utils/training.py:234\u001b[0m, in \u001b[0;36mTrainableDiffusionModel._one_epoch\u001b[0;34m(self, dataloader, end_processor, class_free_guidance_threshhold)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m \tlabels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 234\u001b[0m noise, predicted_noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_step(batch, labels)\n\u001b[1;32m    236\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(noise, predicted_noise)\n\u001b[1;32m    237\u001b[0m end_processor(loss)\n",
      "File \u001b[0;32m~/diffusion_noise_scheduling/utils/training.py:218\u001b[0m, in \u001b[0;36mTrainableDiffusionModel._train_step\u001b[0;34m(self, batch, labels)\u001b[0m\n\u001b[1;32m    216\u001b[0m noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_noise(batch\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    217\u001b[0m noised_videos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnoise_scheduler\u001b[39m.\u001b[39madd_noise(batch, noise, steps)\n\u001b[0;32m--> 218\u001b[0m \u001b[39mreturn\u001b[39;00m noise, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_ref(\n\u001b[1;32m    219\u001b[0m \t\t\tnoised_videos,\n\u001b[1;32m    220\u001b[0m \t\t\tsteps,\n\u001b[1;32m    221\u001b[0m \t\t\tlabels,\n\u001b[1;32m    222\u001b[0m \t\t)\u001b[39m.\u001b[39msample\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/diffusion_noise_scheduling/models/basic_models.py:105\u001b[0m, in \u001b[0;36mMovMNISTModel.forward\u001b[0;34m(self, X, t, classes)\u001b[0m\n\u001b[1;32m    102\u001b[0m     embs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sin_embeddings(X\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    104\u001b[0m \u001b[39m# Passing through main model\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain_model\u001b[39m.\u001b[39;49mforward(sample\u001b[39m=\u001b[39;49mX, timestep\u001b[39m=\u001b[39;49mt, encoder_hidden_states\u001b[39m=\u001b[39;49membs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/unets/unet_3d_condition.py:682\u001b[0m, in \u001b[0;36mUNet3DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, down_block_additional_residuals, mid_block_additional_residual, return_dict)\u001b[0m\n\u001b[1;32m    679\u001b[0m     upsample_size \u001b[39m=\u001b[39m down_block_res_samples[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    681\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(upsample_block, \u001b[39m\"\u001b[39m\u001b[39mhas_cross_attention\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m upsample_block\u001b[39m.\u001b[39mhas_cross_attention:\n\u001b[0;32m--> 682\u001b[0m     sample \u001b[39m=\u001b[39m upsample_block(\n\u001b[1;32m    683\u001b[0m         hidden_states\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m    684\u001b[0m         temb\u001b[39m=\u001b[39;49memb,\n\u001b[1;32m    685\u001b[0m         res_hidden_states_tuple\u001b[39m=\u001b[39;49mres_samples,\n\u001b[1;32m    686\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    687\u001b[0m         upsample_size\u001b[39m=\u001b[39;49mupsample_size,\n\u001b[1;32m    688\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    689\u001b[0m         num_frames\u001b[39m=\u001b[39;49mnum_frames,\n\u001b[1;32m    690\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     sample \u001b[39m=\u001b[39m upsample_block(\n\u001b[1;32m    694\u001b[0m         hidden_states\u001b[39m=\u001b[39msample,\n\u001b[1;32m    695\u001b[0m         temb\u001b[39m=\u001b[39memb,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m         num_frames\u001b[39m=\u001b[39mnum_frames,\n\u001b[1;32m    699\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/unets/unet_3d_blocks.py:807\u001b[0m, in \u001b[0;36mCrossAttnUpBlock3D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, upsample_size, attention_mask, num_frames, cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m     hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m    806\u001b[0m     hidden_states \u001b[39m=\u001b[39m temp_conv(hidden_states, num_frames\u001b[39m=\u001b[39mnum_frames)\n\u001b[0;32m--> 807\u001b[0m     hidden_states \u001b[39m=\u001b[39m attn(\n\u001b[1;32m    808\u001b[0m         hidden_states,\n\u001b[1;32m    809\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    810\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    811\u001b[0m         return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    812\u001b[0m     )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    813\u001b[0m     hidden_states \u001b[39m=\u001b[39m temp_attn(\n\u001b[1;32m    814\u001b[0m         hidden_states,\n\u001b[1;32m    815\u001b[0m         num_frames\u001b[39m=\u001b[39mnum_frames,\n\u001b[1;32m    816\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39mcross_attention_kwargs,\n\u001b[1;32m    817\u001b[0m         return_dict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    818\u001b[0m     )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    820\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:391\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    379\u001b[0m         hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    380\u001b[0m             create_custom_forward(block),\n\u001b[1;32m    381\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         hidden_states \u001b[39m=\u001b[39m block(\n\u001b[1;32m    392\u001b[0m             hidden_states,\n\u001b[1;32m    393\u001b[0m             attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    394\u001b[0m             encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    395\u001b[0m             encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    396\u001b[0m             timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[1;32m    397\u001b[0m             cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    398\u001b[0m             class_labels\u001b[39m=\u001b[39;49mclass_labels,\n\u001b[1;32m    399\u001b[0m         )\n\u001b[1;32m    401\u001b[0m \u001b[39m# 3. Output\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_input_continuous:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/attention.py:400\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     ff_output \u001b[39m=\u001b[39m _chunked_feed_forward(\n\u001b[1;32m    397\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff, norm_hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_size, lora_scale\u001b[39m=\u001b[39mlora_scale\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    399\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     ff_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff(norm_hidden_states, scale\u001b[39m=\u001b[39;49mlora_scale)\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mada_norm_zero\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    403\u001b[0m     ff_output \u001b[39m=\u001b[39m gate_mlp\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m ff_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/attention.py:672\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet:\n\u001b[1;32m    671\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, compatible_cls):\n\u001b[0;32m--> 672\u001b[0m         hidden_states \u001b[39m=\u001b[39m module(hidden_states, scale)\n\u001b[1;32m    673\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m         hidden_states \u001b[39m=\u001b[39m module(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/activations.py:102\u001b[0m, in \u001b[0;36mGEGLU.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, scale: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m):\n\u001b[1;32m    101\u001b[0m     args \u001b[39m=\u001b[39m () \u001b[39mif\u001b[39;00m USE_PEFT_BACKEND \u001b[39melse\u001b[39;00m (scale,)\n\u001b[0;32m--> 102\u001b[0m     hidden_states, gate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(hidden_states, \u001b[39m*\u001b[39;49margs)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(gate)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/lora.py:430\u001b[0m, in \u001b[0;36mLoRACompatibleLinear.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, scale: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    429\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_layer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(hidden_states)\n\u001b[1;32m    431\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    432\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_video = TrainableDiffusionModel(\n",
    "    model_ref = model_video,\n",
    "    optimizer_ref = optimizer_video,\n",
    "    lr_scheduler_ref=lr_scheduler_video,\n",
    "    noise_scheduler = noise_scheduler_video,\n",
    "    criterion = criterion_video,\n",
    "    device=\"cuda:1\",\n",
    "    model_type=\"video\",\n",
    "    cross_att_dim=4,\n",
    "    EMA_start=7500,\n",
    ")\n",
    "\n",
    "# trainer_video.load_weights_from(trainer_image.model_ref)\n",
    "# trainer_video.load_weights_from(trainer_image.model_ref, other_type=\"ema_model\")\n",
    "\n",
    "test_losses = trainer_video.fit(\n",
    "    dataloader = MovMNIST_dataloader,\n",
    "    save_path = \"./models/trained/test/\",\n",
    "    num_epochs = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f22bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [08:20<00:00,  5.01s/it]\n"
     ]
    }
   ],
   "source": [
    "sampler = TrainableDiffusionModel(\n",
    "    model_ref = model_video,\n",
    "    optimizer_ref = optimizer_video,\n",
    "    lr_scheduler_ref=lr_scheduler_video,\n",
    "    noise_scheduler = noise_scheduler_video,\n",
    "    criterion = criterion_video,\n",
    "    device=\"cuda:1\",\n",
    "    model_type=\"video\",\n",
    "    EMA_start=5000,\n",
    ")\n",
    "\n",
    "sampler.load_state(base_dir_path=\"./models/trained/labeled_mov_mnist/\", suffix=\"last\",\n",
    "                   load_optimizer=False, load_lr_sched=False, load_ema_model=True)\n",
    "objects = sampler.sample(num_samples=8,\n",
    "                         video_length=20,\n",
    "                         prompts = torch.tensor([1, 11, 27, 54, 32, 45, 48, 23]),\n",
    "                         override_noise_cov=mixed_noise,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e26bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(objects):\n",
    "    write_video(\n",
    "        f\"./results/MovMNIST/labeled_video/mixed_noise_{i}.mp4\",\n",
    "        v.repeat(3, 1, 1, 1).permute(1, 2, 3, 0),\n",
    "        fps=7,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8e0ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.16it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = sampler.sample(num_samples=8, prompts = torch.tensor([1, 11, 27, 54, 32, 45, 48, 23]))\n",
    "# images = a.sample.detach().cpu()\n",
    "# images = ((images.clip(-1, 1) + 1) / 2 * 255).to(torch.uint8)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=4, constrained_layout=True)\n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "for i, im in enumerate(images):\n",
    "    ax[i // 4][i % 4].imshow(im.permute(1, 2, 0), cmap=\"grey\")\n",
    "    ax[i // 4][i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608210f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, im in enumerate(images):\n",
    "    write_png(im, f\"results/MovMNIST/labeled_frames/_{i}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
