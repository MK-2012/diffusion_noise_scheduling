{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fe4880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koltakovmi/.local/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.noise_gen import *\n",
    "from utils.training import *\n",
    "from utils.dataset_loaders import *\n",
    "from models.basic_models import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import write_video, write_png\n",
    "from diffusers import UNet3DConditionModel, DDPMScheduler, DDPMPipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580790a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b6dff",
   "metadata": {},
   "source": [
    "Creating dataset and dataloader for UCF-101 and MovingMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa625426",
   "metadata": {},
   "outputs": [],
   "source": [
    "MovMNIST_frame_dataset = MovMNISTFrameDataset(\"./datasets/moving_mnist_labeled/\")\n",
    "MovMNIST_frame_dataloader = DataLoader(MovMNIST_frame_dataset, shuffle=True, batch_size=72)\n",
    "\n",
    "dev = \"cuda:5\"\n",
    "\n",
    "model_frame, noise_scheduler_frame, optimizer_frame, lr_scheduler_frame, criterion_frame = init_mov_mnist_model(\n",
    "    lr_warmup_steps=100,\n",
    "    num_epochs=4,\n",
    "    beta_start=1.17e-3,\n",
    "    beta_end=1.88e-1,\n",
    "    object_cnt = len(MovMNIST_frame_dataloader),\n",
    "    device=dev,\n",
    "    model_type=\"image\",\n",
    "    use_labels=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6edd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "MovMNIST_dataset = MovMNISTDataset(\"./datasets/moving_mnist_labeled/\")\n",
    "MovMNIST_dataloader = DataLoader(MovMNIST_dataset, shuffle=True, batch_size=2)\n",
    "\n",
    "dev = \"cuda:0\"\n",
    "\n",
    "model_video, noise_scheduler_video, optimizer_video, lr_scheduler_video, criterion_video = init_mov_mnist_model(\n",
    "    lr_warmup_steps=100,\n",
    "    num_epochs=4,\n",
    "    beta_start=1.17e-3,\n",
    "    beta_end=1.88e-1,\n",
    "    object_cnt = len(MovMNIST_dataloader),\n",
    "    device=dev,\n",
    "    model_type=\"video\",\n",
    "    use_labels=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1a03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_image = TrainableDiffusionModel(\n",
    "    model_ref = model_frame,\n",
    "    optimizer_ref = optimizer_frame,\n",
    "    lr_scheduler_ref=lr_scheduler_frame,\n",
    "    noise_scheduler = noise_scheduler_frame,\n",
    "    criterion = criterion_frame,\n",
    "    device=\"cuda:1\",\n",
    "    model_type=\"image\",\n",
    "    cross_att_dim=4,\n",
    "    EMA_start=2500,\n",
    ")\n",
    "\n",
    "trainer_image.load_state(base_dir_path=\"./models/trained/mov_mnist_frames_batch96/\", suffix=\"8000\",\n",
    "                   load_optimizer=False, load_lr_sched=False, load_ema_model=False)\n",
    "\n",
    "# test_losses = trainer.fit(\n",
    "#     dataloader = MovMNIST_frame_dataloader,\n",
    "#     save_path = \"./models/trained/mov_mnist_frames_batch96/\",\n",
    "#     num_epochs = 4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c100056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                   | 1/5000 [00:02<4:08:55,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0952, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 132.25 MiB is free. Process 3376121 has 170.00 MiB memory in use. Process 827049 has 986.00 MiB memory in use. Process 832669 has 104.00 MiB memory in use. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 10.17 GiB is allocated by PyTorch, and 79.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/koltakovmi/diffusion_noise_scheduling/Analysis notebook.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer_video \u001b[39m=\u001b[39m TrainableDiffusionModel(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model_ref \u001b[39m=\u001b[39m model_video,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     optimizer_ref \u001b[39m=\u001b[39m optimizer_video,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# trainer_video.load_weights_from(trainer_image.model_ref)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# trainer_video.load_weights_from(trainer_image.model_ref, other_type=\"ema_model\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m test_losses \u001b[39m=\u001b[39m trainer_video\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     dataloader \u001b[39m=\u001b[39;49m MovMNIST_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     save_path \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m./models/trained/test/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     num_epochs \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m )\n",
      "File \u001b[0;32m~/diffusion_noise_scheduling/utils/training.py:271\u001b[0m, in \u001b[0;36mTrainableDiffusionModel.fit\u001b[0;34m(self, dataloader, save_path, num_epochs, end_processor, grad_accum_steps, class_free_guidance_threshhold)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39m# Going through epochs\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 271\u001b[0m \tnew_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_one_epoch(\n\u001b[1;32m    272\u001b[0m \t\tdataloader, end_proc,\n\u001b[1;32m    273\u001b[0m \t\tclass_free_guidance_threshhold\u001b[39m=\u001b[39;49mclass_free_guidance_threshhold\n\u001b[1;32m    274\u001b[0m \t)\n\u001b[1;32m    275\u001b[0m \tlosses \u001b[39m=\u001b[39m cat([losses, as_tensor(new_losses)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    277\u001b[0m \u001b[39m# saving last versions of models\u001b[39;00m\n",
      "File \u001b[0;32m~/diffusion_noise_scheduling/utils/training.py:241\u001b[0m, in \u001b[0;36mTrainableDiffusionModel._one_epoch\u001b[0;34m(self, dataloader, end_processor, class_free_guidance_threshhold)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m \tlabels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 241\u001b[0m noise, predicted_noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_step(batch, labels)\n\u001b[1;32m    243\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m zeros\n\u001b[1;32m    244\u001b[0m zer \u001b[39m=\u001b[39m zeros(predicted_noise\u001b[39m.\u001b[39mshape, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/diffusion_noise_scheduling/utils/training.py:225\u001b[0m, in \u001b[0;36mTrainableDiffusionModel._train_step\u001b[0;34m(self, batch, labels)\u001b[0m\n\u001b[1;32m    223\u001b[0m noise \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_noise(batch\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    224\u001b[0m noised_videos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnoise_scheduler\u001b[39m.\u001b[39madd_noise(batch, noise, steps)\n\u001b[0;32m--> 225\u001b[0m \u001b[39mreturn\u001b[39;00m noise, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_ref(\n\u001b[1;32m    226\u001b[0m \t\t\tnoised_videos,\n\u001b[1;32m    227\u001b[0m \t\t\tsteps,\n\u001b[1;32m    228\u001b[0m \t\t\tlabels,\n\u001b[1;32m    229\u001b[0m \t\t)\u001b[39m.\u001b[39msample\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/diffusion_noise_scheduling/models/basic_models.py:69\u001b[0m, in \u001b[0;36mMovMNISTModel.forward\u001b[0;34m(self, X, t, classes)\u001b[0m\n\u001b[1;32m     65\u001b[0m     embs \u001b[39m=\u001b[39m zeros(X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_att_dim,\n\u001b[1;32m     66\u001b[0m                  device\u001b[39m=\u001b[39m\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters())\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     68\u001b[0m \u001b[39m# Passing through main model\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain_model\u001b[39m.\u001b[39;49mforward(sample\u001b[39m=\u001b[39;49mX, timestep\u001b[39m=\u001b[39;49mt,\n\u001b[1;32m     70\u001b[0m                                encoder_hidden_states\u001b[39m=\u001b[39;49membs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/unets/unet_3d_condition.py:620\u001b[0m, in \u001b[0;36mUNet3DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, down_block_additional_residuals, mid_block_additional_residual, return_dict)\u001b[0m\n\u001b[1;32m    617\u001b[0m sample \u001b[39m=\u001b[39m sample\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mreshape((sample\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m num_frames, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m sample\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m:])\n\u001b[1;32m    618\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_in(sample)\n\u001b[0;32m--> 620\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_in(\n\u001b[1;32m    621\u001b[0m     sample,\n\u001b[1;32m    622\u001b[0m     num_frames\u001b[39m=\u001b[39;49mnum_frames,\n\u001b[1;32m    623\u001b[0m     cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    624\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    625\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    627\u001b[0m \u001b[39m# 3. down\u001b[39;00m\n\u001b[1;32m    628\u001b[0m down_block_res_samples \u001b[39m=\u001b[39m (sample,)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/transformers/transformer_temporal.py:176\u001b[0m, in \u001b[0;36mTransformerTemporalModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, class_labels, num_frames, cross_attention_kwargs, return_dict)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m# 2. Blocks\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 176\u001b[0m     hidden_states \u001b[39m=\u001b[39m block(\n\u001b[1;32m    177\u001b[0m         hidden_states,\n\u001b[1;32m    178\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    179\u001b[0m         timestep\u001b[39m=\u001b[39;49mtimestep,\n\u001b[1;32m    180\u001b[0m         cross_attention_kwargs\u001b[39m=\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    181\u001b[0m         class_labels\u001b[39m=\u001b[39;49mclass_labels,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    184\u001b[0m \u001b[39m# 3. Output\u001b[39;00m\n\u001b[1;32m    185\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_out(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/attention.py:400\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     ff_output \u001b[39m=\u001b[39m _chunked_feed_forward(\n\u001b[1;32m    397\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff, norm_hidden_states, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_size, lora_scale\u001b[39m=\u001b[39mlora_scale\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    399\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 400\u001b[0m     ff_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff(norm_hidden_states, scale\u001b[39m=\u001b[39;49mlora_scale)\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mada_norm_zero\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    403\u001b[0m     ff_output \u001b[39m=\u001b[39m gate_mlp\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m ff_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/attention.py:672\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet:\n\u001b[1;32m    671\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, compatible_cls):\n\u001b[0;32m--> 672\u001b[0m         hidden_states \u001b[39m=\u001b[39m module(hidden_states, scale)\n\u001b[1;32m    673\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m         hidden_states \u001b[39m=\u001b[39m module(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/activations.py:102\u001b[0m, in \u001b[0;36mGEGLU.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, scale: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m):\n\u001b[1;32m    101\u001b[0m     args \u001b[39m=\u001b[39m () \u001b[39mif\u001b[39;00m USE_PEFT_BACKEND \u001b[39melse\u001b[39;00m (scale,)\n\u001b[0;32m--> 102\u001b[0m     hidden_states, gate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(hidden_states, \u001b[39m*\u001b[39;49margs)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(gate)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/diffusers/models/lora.py:430\u001b[0m, in \u001b[0;36mLoRACompatibleLinear.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, scale: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    429\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_layer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(hidden_states)\n\u001b[1;32m    431\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    432\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 160.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 132.25 MiB is free. Process 3376121 has 170.00 MiB memory in use. Process 827049 has 986.00 MiB memory in use. Process 832669 has 104.00 MiB memory in use. Including non-PyTorch memory, this process has 10.39 GiB memory in use. Of the allocated memory 10.17 GiB is allocated by PyTorch, and 79.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer_video = TrainableDiffusionModel(\n",
    "    model_ref = model_video,\n",
    "    optimizer_ref = optimizer_video,\n",
    "    lr_scheduler_ref=lr_scheduler_video,\n",
    "    noise_scheduler = noise_scheduler_video,\n",
    "    criterion = criterion_video,\n",
    "    device=\"cuda:0\",\n",
    "    model_type=\"video\",\n",
    "    EMA_start=7500,\n",
    "    \n",
    ")\n",
    "\n",
    "# trainer_video.load_weights_from(trainer_image.model_ref)\n",
    "# trainer_video.load_weights_from(trainer_image.model_ref, other_type=\"ema_model\")\n",
    "\n",
    "test_losses = trainer_video.fit(\n",
    "    dataloader = MovMNIST_dataloader,\n",
    "    save_path = \"./models/trained/test/\",\n",
    "    num_epochs = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f22bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [08:25<00:00,  5.06s/it]\n"
     ]
    }
   ],
   "source": [
    "sampler = TrainableDiffusionModel(\n",
    "    model_ref = model_video,\n",
    "    optimizer_ref = optimizer_video,\n",
    "    lr_scheduler_ref=lr_scheduler_video,\n",
    "    noise_scheduler = noise_scheduler_video,\n",
    "    criterion = criterion_video,\n",
    "    device=dev,\n",
    "    model_type=\"video\",\n",
    "    EMA_start=5000,\n",
    ")\n",
    "\n",
    "sampler.load_state(base_dir_path=\"./models/trained/labeled_mov_mnist_mixed_noise/\", suffix=\"last\",\n",
    "                   load_optimizer=False, load_lr_sched=False, load_ema_model=True)\n",
    "objects = sampler.sample(num_samples=8,\n",
    "                         video_length=20,\n",
    "                         prompts = torch.tensor([1, 11, 27, 54, 32, 45, 48, 23]),\n",
    "                         override_noise_cov=progressive_noise,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e26bdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GIF muxer supports only a single video GIF stream.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[Errno 22] Invalid argument: './results/MovMNIST/labeled_video_mixed_noise/prog_noise_0.gif'; last error log: [gif] GIF muxer supports only a single video GIF stream.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/koltakovmi/diffusion_noise_scheduling/Analysis notebook.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(objects):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     write_video(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./results/MovMNIST/labeled_video_mixed_noise/prog_noise_\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m.gif\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         v\u001b[39m.\u001b[39;49mrepeat(\u001b[39m3\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         fps\u001b[39m=\u001b[39;49m\u001b[39m7\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Balmaren/home/koltakovmi/diffusion_noise_scheduling/Analysis%20notebook.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/io/video.py:138\u001b[0m, in \u001b[0;36mwrite_video\u001b[0;34m(filename, video_array, fps, video_codec, options, audio_array, audio_fps, audio_codec, audio_options)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m# Flush stream\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39mfor\u001b[39;00m packet \u001b[39min\u001b[39;00m stream\u001b[39m.\u001b[39mencode():\n\u001b[0;32m--> 138\u001b[0m     container\u001b[39m.\u001b[39;49mmux(packet)\n",
      "File \u001b[0;32mav/container/output.pyx:211\u001b[0m, in \u001b[0;36mav.container.output.OutputContainer.mux\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/container/output.pyx:217\u001b[0m, in \u001b[0;36mav.container.output.OutputContainer.mux_one\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/container/output.pyx:183\u001b[0m, in \u001b[0;36mav.container.output.OutputContainer.start_encoding\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/container/core.pyx:285\u001b[0m, in \u001b[0;36mav.container.core.Container.err_check\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mav/error.pyx:336\u001b[0m, in \u001b[0;36mav.error.err_check\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [Errno 22] Invalid argument: './results/MovMNIST/labeled_video_mixed_noise/prog_noise_0.gif'; last error log: [gif] GIF muxer supports only a single video GIF stream."
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(objects):\n",
    "    write_video(\n",
    "        f\"./results/MovMNIST/labeled_video_mixed_noise/prog_noise_{i}.gif\",\n",
    "        v.repeat(3, 1, 1, 1).permute(1, 2, 3, 0),\n",
    "        fps=7,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8e0ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                              | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.16it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLsAAALHCAYAAABmNkB7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdnUlEQVR4nO3d23baWAJFUcOo//9k0U81uqIoybGi69Kcb0kbrLSxgFXy9uvz+Xy+AAAAACDgffYBAAAAAMBWxC4AAAAAMsQuAAAAADLELgAAAAAyxC4AAAAAMsQuAAAAADLELgAAAAAyxC4AAAAAMv4Z/cDX67XncQAM+3w+v/3fna+Aq3C+Au7C+Qq4iz+dr76+XNkFAAAAQIjYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAxj9nHwBczefz2eR+Xq/XJvcDAAAAjHNlFwAAAAAZYhcAAAAAGWIXAAAAABk2uwAAuJU1+5q2NAHgOVzZBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECGgXoebc3ALQAAAD+apumHPy/9YpC177/m9+WXjvAnruwCAAAAIEPsAgAAACBD7AIAAAAgw2YXbMTPjQMAAE81fz+0tM+19j3T/L62vG+aXNkFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIaBeh5lachwDeOHAHCMtc/dnqsB9jNybl46Dy/93TRNq24Hv+PKLgAAAAAyxC4AAAAAMsQuAAAAADJsdpG11T4XAAAAvzd//zW6s/V+/3gNjvdxbMGVXQAAAABkiF0AAAAAZIhdAAAAAGSIXQAAAABkGKiHAaPjigDAekaJAa5pfn4eOV97D8WZXNkFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIaBejKM2gLAvYw8dy99zHz02AgywL7m5+I9z7tL9+29Ht/lyi4AAAAAMsQuAAAAADLELgAAAAAybHbBjN0PALiOpedlz9UAxzryvGufiy24sgsAAACADLELAAAAgAyxCwAAAIAMsQsAAACADAP1AADcyny82GA9wL5Gzrt7DsuP3LfnBv7LlV0AAAAAZIhdAAAAAGSIXQAAAABk2Ozi0fwcNwDcz8jz957bMSO8xgBK5ue0pXPs+73uWpqR87VzKt/lyi4AAAAAMsQuAAAAADLELgAAAAAyxC4AAAAAMgzUAwCwu7WD8XcYo18yPybjysBdTNP009/Nz2lrx+jXck7lu1zZBQAAAECG2AUAAABAhtgFAAAAQIbNLgAIWdrZWOPoLY415vsda3eb7vBvfbIr7nEBlC09L251Ll77OmW+0bV0PCMf86fb0OHVHQAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZBuoBgFsySH8vSyPAWw0ejwwVjxzTnmP4a48R4ArWnB/XnlNHni/WPqfMb7c0mO91QoOvIgAAAAAZYhcAAAAAGWIXAAAAABk2u+BAe26BbMV+CHBFS5saR+4tcW1bPXeN3I/HGfBEI1tXI8/LzrMcxZVdAAAAAGSIXQAAAABkiF0AAAAAZIhdAAAAAGQYqAd+MDIIacQe+ubDs+/3sf99bGn4ds6ALd+x54i9xyJQNz/PjZxTR8+7a+57xMj9HP36huP4ygIAAACQIXYBAAAAkCF2AQAAAJAhdgEAAACQYaCeR1salDW+DtBheJYr22uUGeBORs59I+fLLUfzuT+vAAEAAADIELsAAAAAyBC7AAAAAMiw2QUXs7QjNuLInz+fpumnv7OLA217ft8v3TfcydJz8Nrnc4ArGnnOn5/31p4Hlz7Xmvuyz/Vs3p0CAAAAkCF2AQAAAJAhdgEAAACQIXYBAAAAkGGgHnay5dD8/L7OHls8+/MDvzYfdd1z/N2wPHdyxcF4z6dAydXOaUvn/asdI/txZRcAAAAAGWIXAAAAABliFwAAAAAZNrvIGPn56z33Oo7cAvHz5wAUzJ+7rrirtdbI3ubVNjkB7mx+Tp3vmPIsvvoAAAAAZIhdAAAAAGSIXQAAAABkiF0AAAAAZBio51G2Gn5dO6C7dojWYC2w1sg46zRNBxwJ/Nno890dhuw9dwNsZ+QXmqz5GLpc2QUAAABAhtgFAAAAQIbYBQAAAECGzS44kJ8RB65oadfLjhdX5vkUAPgdV3YBAAAAkCF2AQAAAJAhdgEAAACQIXYBAAAAkGGgHgZ8Pp+zDwHgUEuj9XNHjtiPHA8AwL+8h3s2rxwBAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyDBQDzNnDxm+Xq9Vtzv7uIHnWRqN32q03iA9APAra98z8RxeSQIAAACQIXYBAAAAkCF2AQAAAJBhs4tHO3vn6q77XH5GHvj62m6fCwAAtuTKLgAAAAAyxC4AAAAAMsQuAAAAADLELgAAAAAyDNTDgbYapF8aqDcaDwAAAK7sAgAAACBE7AIAAAAgQ+wCAAAAIMNmF4+ytHV1prXHc+Q+ly0w4Ovr62uapt3u+/32394AANiOV5cAAAAAZIhdAAAAAGSIXQAAAABkiF0AAAAAZBiohwONDNIvfcx8JH7kftYOyxukB/ZmkB4AgD15tQkAAABAhtgFAAAAQIbYBQAAAECGzS64mCO3tuxzAQAAUOPKLgAAAAAyxC4AAAAAMsQuAAAAADLELgAAAAAyDNTDzOfz+env1gy5b3U/S44csQeeaZqmTe7n/fbf1YB9Lb3mWsPrJIAOr0ABAAAAyBC7AAAAAMgQuwAAAADIsNkFM2fvat3l8wFt83PKVps4AH/DuYi/NfIYGtneHd3nnX/cyGt2r+vh77myCwAAAIAMsQsAAACADLELAAAAgAyxCwAAAIAMA/VwA0YqgaONDOoujfO+3/47GrAdg/RsbXRY/k+3G319vua+p2n64/2MfG94TubJPPoBAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyDBQz6OsGXYEeCKjtgAUrHm9v2ZU/m/uaw2/wAp+zytZAAAAADLELgAAAAAyxC4AAAAAMmx28WhLP+tuxwuom6bp27ex4QXU2Dx6hvlr+7Vf97W3G3lvYVcYtueVKwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZBuphxlgpAMDx9hzl9vqOf619nI38cpe1v/xqfruR+/GLtuD3XNkFAAAAQIbYBQAAAECG2AUAAABAhs0uAHg4ux/AGbY6z8zv5/3++b/n2wN7rrO/Pkd+/rP/rXAlruwCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIMNAPQA8zNJ4M0DF0hj90t9tNeY9v28j4fzOyONj5PG69DGe3+H/fDcAAAAAkCF2AQAAAJAhdgEAAACQYbMLAAC4rflO0Z77XCPO/vz8aGTras3u2pabWdM0/fG+PYbge1zZBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECGgXoAAGBXS4PbT7JmAJ19XPH/+7WD+MCvubILAAAAgAyxCwAAAIAMsQsAAACADLELAAAAgAwD9QAAwG3dcbh76ZivOJzO9pa+9iO/wMDjA77HlV0AAAAAZIhdAAAAAGSIXQAAAABk2OwCAAA4mR2v51rzdfbYgN9zZRcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGQbqAQAALmg+Wm+U/H6WfvHAmtv42sP3uLILAAAAgAyxCwAAAIAMsQsAAACADJtdAAAAcBFL+1w2u+B7XNkFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIaBegAA4PHWDoB/Pp+NjwSAv+XKLgAAAAAyxC4AAAAAMsQuAAAAADJsdgEAAI8zstF19h7X2h0xzrHV48XXHf6eK7sAAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyDBQDwAAPM7Z4/Pc29LjZ+nvRsbmDdLD9lzZBQAAAECG2AUAAABAhtgFAAAAQIbNLgAAYFcjm0RP39Cy23Qva/e5gGO4sgsAAACADLELAAAAgAyxCwAAAIAMsQsAAACADAP1AADA6Z40Ym/I/H4qjz14Cld2AQAAAJAhdgEAAACQIXYBAAAAkCF2AQAAAJBhoB4AAGAjxuf5r/mw/fvtehM4gu80AAAAADLELgAAAAAyxC4AAAAAMmx2AQAAt2APi7PMt7c8FuHaXNkFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIaBegAAAPiN99t1InAnvmMBAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyBC7AAAAAMh4fT6fz9kHAQAAAABbcGUXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZ/4x+4Ov12vM4AIZ9Pp/f/u/OV8BVOF8Bd+F8BdzFn85XX1+u7AIAAAAgROwCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAg45+zDwAAAADY1jRNm93X++06Ge7FIxYAAACADLELAAAAgAyxCwAAAIAMm10AAABwI1vucUGRK7sAAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyDBQDwAAADf3er1++PPn8xm63fvtGhh6PKoBAAAAyBC7AAAAAMgQuwAAAADIsNkFADc1usVxdfONEQDg++avCzy/8mSu7AIAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAgw0A9ANxAZYweAPi+aZq+fZu1rx2WPtf77ToZ7sUjFgAAAIAMsQsAAACADLELAAAAgAyxCwAAAIAMA/UAcLKRAdmlj3m9XnsczuHK/zYAuLo1r0OWbmPEnivxaAQAAAAgQ+wCAAAAIEPsAgAAACDDZhcA3MDaDau1txvZ71i675HbAQDXsdVz9zRNP/zZhhdn8ugDAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIMNAPQAcbKsh2LXj83ve9/x2BusB4Jnmg/W/YsiePXhUAQAAAJAhdgEAAACQIXYBAAAAkGGz62FGf256jflOy55bMgAAAABLXNkFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIaBen6yNCz/+Xz+eLv5xyyN4b/f+irwLCPnzxF+6QcAPNf8fdSev3gMCpQHAAAAADLELgAAAAAyxC4AAAAAMmx2hY3+HPeROzB2vAAAAK5j5P3YnhthW92395X8l0cDAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABkG6h9maYz+8/n88OelYb89BwkB+NmRvzwEAHiu+Xs9Q+8UeBQDAAAAkCF2AQAAAJAhdgEAAACQIXYBAAAAkGGgPmRkRH4+Rv/1NTaCPDJSuHbE3iAiULJ0ngUA2NLRv1Rs/vmu+AvMvK/kv3z1AQAAAMgQuwAAAADIELsAAAAAyLDZxWb7MiPbX7ZsAH42cv4EADjLkXtgR39+mlzZBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECGgfqb2nKgb2QQcMR8YNkYPcDzGJAFgHPM39c97Tl5q/e1NHg0AAAAAJAhdgEAAACQIXYBAAAAkGGz6ybu+PPWNrsA2rY6z883HwGA85z93vPsz0+DK7sAAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyDBQf1NLY74jQ8Hv9/X6pmFigOvzS0cAgD0YpGcP1ysfAAAAALCS2AUAAABAhtgFAAAAQIbNrpu6w3bK6K7Ymn/LVj/XfcUNM4CjHf2cYqsRAPa39F7HPhZP4Z0+AAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABkG6sOOHl+fjx2uHTweGU0cHb8HuIOl89dWI+5nnxuN0QMAW/OLxvgTjxAAAAAAMsQuAAAAADLELgAAAAAybHbdxNV+JnlkV2tL882XkX0bOzHAnY1sbY2cC/fcA3OeBYB7GXlfefR7vRFXez/M9XnEAAAAAJAhdgEAAACQIXYBAAAAkCF2AQAAAJBhoJ5bmA8sL40iG0oGnmbkvGeMHgD4jj1H7A3NcxSPNAAAAAAyxC4AAAAAMsQuAAAAADLELgAAAAAyDNQzZO0A4QgjhQD7MSwPAMDTqAwAAAAAZIhdAAAAAGSIXQAAAABk2OziJ1vtcy3txNiOARjz+Xx++LPzJwBwlj03nGEPruwCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIMNAPbsxpgzUzEfj53/e0lbn0KVjdH4GAH5lqzH699u1NZzHow8AAACADLELAAAAgAyxCwAAAIAMm10APN7a7a2R7as9t7cAAH5lq+2tUTa6uBKPRgAAAAAyxC4AAAAAMsQuAAAAADLELgAAAAAyDNQDwIKlQfgjB+lH7nftaP38dnsdM3CeO/5SC+ci+DtHD9LDlbmyCwAAAIAMsQsAAACADLELAAAAgAybXWzm/dZOgXsY2bJZ2o6xJwNc0R33uZas3UqEJ7jDHtf8GL0/5EwefQAAAABkiF0AAAAAZIhdAAAAAGSIXQAAAABkGKgHIG3tcPPVRpErA9QA3zE/913t3AxbuMP4PNyNK7sAAAAAyBC7AAAAAMgQuwAAAADIELsAAAAAyDBQz0/ebw0U4GwG6YErWjsQv9U5beR+jNjD94y8/1szor90G+81OYpHGgAAAAAZYhcAAAAAGWIXAAAAABk2uwDgYGfvcdmzgZY7fE8vHeNe58Kl+73D/0dwhLWbWfPbrdnw+tXt7HixB48qAAAAADLELgAAAAAyxC4AAAAAMsQuAAAAADIM1APAgpGB47OH5gHuzDkVtmXoHf7PdwMAAAAAGWIXAAAAABliFwAAAAAZNrsAYNAd92TmmzgAV3HHcyrs4Y5bW3c8Zp7FIxQAAACADLELAAAAgAyxCwAAAIAMsQsAAACADAP1cKBpmv74McYegb9hkB5Y8qQxeOdBALyrBgAAACBD7AIAAAAgQ+wCAAAAIMNmF482sqG1ZKtdLZsSsL+l77Mjt2tGvs+ftKUD8K/5uW/kfOm1EwAjXNkFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIaBeh5l7SD9XoxSwznOHjje6nv/7H8HwKil855BegD24souAAAAADLELgAAAAAyxC4AAAAAMmx2kbV2n2tpG2LNXsTS55/fj80u6LPPBVzB0jnkyNchI59/z/Pc2a+5nMMBjuXKLgAAAAAyxC4AAAAAMsQuAAAAADLELgAAAAAyDNSTsXaQfm7PAdMjh1iBcxikB+5iq/PM2vPeyOc/e1gegHtyZRcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGQbqeZT5EOrS6On7fVwDXvr8a4f2jzxuYFvG6IE7Wzs0f7Vf3LN0jGuO6ex/BwCu7AIAAAAgROwCAAAAIEPsAgAAACDDZhe3NLJrtbSXsLTFULFm68vOF/yd8jkFYG8jW6p77l+t3RoD4Pq80wUAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAgw0A9zKwZegcAuJuR8fU1A/FrR91HfrnQ0sfsOWIPwD25sgsAAACADLELAAAAgAyxCwAAAIAMm13cwnxHa2TTYa2R3YetPteW5se9dIw2LeAafC8CJXu+LpqfL/c8f17x9R0A67iyCwAAAIAMsQsAAACADLELAAAAgAyxCwAAAIAMA/Xc0siA6NqR0bsOu9/hGKHG9x1QVxltP3JEH4DzubILAAAAgAyxCwAAAIAMsQsAAACADJtd3ML7/ecuO03Tt2+zpfnnX9pvmO9FHH2MAABXddfdVACuxzttAAAAADLELgAAAAAyxC4AAAAAMsQuAAAAADIM1JNxtbH3pZFVAICrmI+/7/naZWRofuTzb/UxaxnMB7iHa9UBAAAAAPgLYhcAAAAAGWIXAAAAABliFwAAAAAZBuoBAIDF8fWRsferjbZf7XgAOJ4ruwAAAADIELsAAAAAyBC7AAAAAMiw2QUHer/1ZQDgPo7cvxrZDFvaENvzGO1/AdyTd94AAAAAZIhdAAAAAGSIXQAAAABkiF0AAAAAZBioh50YNAUA+Dsjr6eWRusBeDZXdgEAAACQIXYBAAAAkCF2AQAAAJBhswsAAHg8e6sAHa7sAgAAACBD7AIAAAAgQ+wCAAAAIEPsAgAAACDDQD1sxKgpAMB9eO0G0OXKLgAAAAAyxC4AAAAAMsQuAAAAADJsdgEAALfw+XzOPgQAbsCVXQAAAABkiF0AAAAAZIhdAAAAAGSIXQAAAABkGKiHjcwHU1+v10lHAgDQsNUgvddlAM/iyi4AAAAAMsQuAAAAADLELgAAAAAybHbBTrbamAAAeAKvnQDYiiu7AAAAAMgQuwAAAADIELsAAAAAyBC7AAAAAMgwUA8beb+1YwCAs71er7MPAYCTeXcOAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAECG2AUAAABAhtgFAAAAQIbYBQAAAEDGP2cfAAAAwOv1OvsQAIhwZRcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGWIXAAAAABliFwAAAAAZYhcAAAAAGa/P5/M5+yAAAAAAYAuu7AIAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIEPsAgAAACBD7AIAAAAgQ+wCAAAAIEPsAgAAACDjfzN1kw1N0Fe2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x800 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = sampler.sample(num_samples=8, prompts = torch.tensor([1, 11, 27, 54, 32, 45, 48, 23]))\n",
    "# images = a.sample.detach().cpu()\n",
    "# images = ((images.clip(-1, 1) + 1) / 2 * 255).to(torch.uint8)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=4, constrained_layout=True)\n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "for i, im in enumerate(images):\n",
    "    ax[i // 4][i % 4].imshow(im.permute(1, 2, 0), cmap=\"grey\")\n",
    "    ax[i // 4][i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608210f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, im in enumerate(images):\n",
    "    write_png(im, f\"results/MovMNIST/labeled_frames/_{i}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
